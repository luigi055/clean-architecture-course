#Clean Architecture:

Clean architecture is a set of practices used to create modern software architecture that is simple, understandable, flexible, testable and maintainable.

Clean Architecture is a more modern replacement for the traditional three.

## What software architecture really is?

It's not easy to define. There are a few general concepts that we can probably all agree upon.

1. It's High-Level.
2. It has to do with the **Structure** of software or how things are organized.
3. It typically involves **layers** which are vertical partitions of the system.
4. It typically involves **Components** of some kind, which are typically horizontal partitions within each layer.
5. It involves the relationshops between these things, that is how they're all wired together.

## Levels of architectural Abstraction

When we're discussing architecture we can focus on one of many levels of abstraction. Starting at the top, that is the most abstract representation of software.

At the top we have the **System**, which can be represented as a set of one or more **Subsystems**, which are typically divided into one or more **Layers** which are often subdivided into one or more **Components**, which contain **Classes** that contain **data** and **methods**.

## What is Bad Architecture?

- It's complex, but due to accidental complexity rather that necessary complexity.
- It's incoherent in the sense that the parts don't seem like they fit together.
- It's rigid, that is the architecture resists change or makes it difficult to evolve the architecture over time.
- It's brittle, touching a part of the code over here might break another part of the code somewhere else.
- It's untestable, that is,you'd really like to write unit tests and integration tests, but the architecture fights you eaxch step of the the way.
- ultimately all of these lead to an architecture that's unmaintainable over the life of the project.

## What is a Good Architecture?

- It's simple or at least it's only as complex as is necessary, and that complexity is not accidental.
- It's understandable, that is, it's easy to reason about the software as a whole.
- It's flexible, we can easily adapt the system to meet changing requirements.
- It's emergent, the architecture evolves over the life of the project.
- It's testable, the architecture makes testing easier, not harder.
- And ultimately all of this leads to an architecture that's more maintainable over the life of the project.

The way we could sum up clean architecture is that it's architecture that is designed for the inhabitants of the architecture... Not for the architect... or the machine.

Clean architecture is a philosophy of architectural essentialism. It's about focusing on what is truly essential to the software's architecture versus what is just implementation detail.

By desiging for the inhabitants we mean the people that will be living within the architecture during the life of the project. This means the users of the system, the developers building the system and the developers maintaining the system.

By not designing for the architect, we mean that the architect should put aside his of her own desires, preferences and wishes, and only consider what is best for the inhabitants of the architecture with each decision that is made.

By not designing for the machine we mean that we should optimize the architecture first for the needs of the inhabitants, that is the users and the developers, and only optimize for the machine when the cost of performance issues to the users, who are inhabitants of the architecture, outweights the benefit of a clean design to the developers who are also inhabitants of the architecture.

Essentially,we want to avoid premature optimization, which as visionary computer scientists Donald Knuth says, is the root of all evil in software development.

## Why invest in Clean Architecture?

- Cost/Benefits.
- Minimize Cost.
- Maximize Value.
- Maximize ROI.

Clean Architecture attemps to do this in several ways.

- First, clean architecture focuses on the essential needs of the primary inhabitants of the system; that are the users. We attempt to build a system that mirrors the use cases and mental models of the users by embedding these use cases and mental models in both our architecture and our code

- Second, clean architecture builds only what is necessary when it is necessary. We attempts to create only the features and corresponding architecture that are necessary to solve the immediate needs of the users in order of each features perceived business value. We attemp to do this without creating any accidental complexity, unnecessary features, premature performance optimizations or architectural embellishments. This helpsto reduce the cost of creating the system.

- Third, clean architecture optimizes for maintainability. For an average enterprise application with a sufficiently long lifecycle, way 10 years or so, we spend significantly more time and money maintaining the system than we do creating it. Several sources that I've seen indicate that roughly 60 to 80% of the cost of the life of the software application comes from maintenance. We optimize for maintainability, which clean architecture does, we in theory reduce the cost of maintaining the system.

In clean Architecture the **Context** is king.
**All decisions are a tradeoff.**
**Align with bussiness goals.**
**Use your best judgement**

# Domain-Centric Architecture.

Here we have the classic three-layer database-centric architecture. Its key feature is that the user interface, business logic, and data access layer revolve around the database. The database is essential, and thus, it's at the center of this architecture. However, a new perspective has changed the way many of us look at our architecture. Rather than having the database at the center of our architecture, some of us are putting the domain at the center, and making the database just an implementation detail outside of the architecture.

Here the domain is essential, and the database is just a detail. So why has this happened? I think this change in perspective is best summed up by a quote from Robert C. Martin, better known in the software industry as Uncle Bob. He says, "the first concern of the architect is to make sure that the house is usable, it is not to ensure that the house is made of brick. " This change in architectural perspective is being caused by a change in perspective about what is essential in an architecture versus what is just an implementation detail. Using our building architecture metaphor, when we're building a house what is essential versus what is a detail?

- The space inside of a house is essential. Without empty space to inhabit the house would serve no purpose.
- The usability of the house is essential. If the house didn't contain rooms and features to support our primary needs, again, the house would not serve its purpose.
- However, the building material is just an implementation detail. We could build it out of brick, stone, wood or many other materials.
- In addition, the ornamentation is just a detail. We could still live in this house whether it had Victorian molding, gold trim, French doors or no ornamentation at all.

The things that are essential in a house are so because they support the primary needs of the inhabitants of the house. Everything else is just an implementation detail. In clean architecture the same holds true. What is essential are the things that support the primary needs of the inhabitants of the architecture.

- The domain model is essential. Without it the system would not represent the mental models of the users.
- The use cases are essential. Without them the system would not solve the user's problems.
- However, the presentation is just a detail. We can deliver the UI in web forms, ASP. NET MVC or as a single page JavaScript application.
- The persistence is just a detail. We can store the data in a relational database, no a SQL database, or as plain old JSON files.

Now don't get me wrong, the presentation and persistence technologies are very important. They are just not essential to solving the problem that the user is attempting to solve with the application. Once we've changed our perspective about what is essential versus what is a detail in software architecture, we can start to see why a transition from database-centric architecture to domain-centric architecture is occurring. With database-centric architectures the database is essential, so the database is at the center of the application, and all dependencies point towards the database. With domain-centric architectures the domain and use cases are essential, and the presentation and persistence are just a detail, so the domain is at the center of the application wrapped in an application layer and all dependencies point towards the domain. So now let's take a look at a few types of domain-centric architectures.

1.  First, we have Alistair Cockburn's hexagonal architecture. It's a layered architecture with the application layer, and thus transitively, the domain at the center of the architecture. In addition, it's a plugin architecture which includes ports and adapters. Essentially, the outer layers of the architecture are adapting the inner application layer to the various presentation mediums, persistence mediums, and external systems. You can run, and thus test, an isolation in this entire application architecture without a UI, a database or any external dependencies.

2.  Next, with the onion architecture by Jeffrey Palermo. This is also a layered architecture with the domain at the center surrounded by an application layer. The outer layers consist of a thin UI as a presentation layer, and an infrastructure layer, which includes persistence. In addition, all dependencies point towards the center of the architecture, that is no inner layer knows about any outer layer. Once again, you can test this application architecture in isolation without a UI, a database or any external dependencies.
3.  Finally, we have the clean architecture by Uncle Bob. Once again, it's a layered architecture with the domain, that is the entities, at the center surrounded by an application layer, that is the use cases. The outer layer consists of ports and adapters adapting the application core to the external dependencies via controllers, gateways, and presenters. In addition, Uncle Bob goes one step further by incorporating Ivar Jacobson's BCE architecture pattern to explain how the presentation layer and the application layer should be wired up.

All three of these architectures have roughly the same benefits. However, as Mark Seaman has pointed out, all of these domain-centric architectures are essentially just focusing on different aspects of the same key set of ideas. Essentially, the all put the domain model at the center, wrap it in an application layer, which embeds the use cases, adapts the application to the implementation details, and all dependencies should point inwards towards the domain. So why would we want to use a domain-centric architecture?

1.  First, focus is placed on the domain, which is essential to the inhabitants of the architecture, that is the users and the developers which, as we've learned in the previous module, provides several benefits and cost reductions for our software.
2.  Second, there is less coupling between the domain logic and the implementation details, for example, the presentation, database, and operating system. This allows the system to be more flexible and adaptable, and we can much more easily evolve the architecture over time.
3.  Third, using a domain-centric architecture allows us to incorporate Domain Driven Design, which is a great set of strategies by Eric Evans for handling business domains with a high degree of complexity.

So why would we not want to use a domain-centric architecture?

1. First, change is difficult. Most developers come out of college having only been taught the traditional three layer database-centric architecture. In addition, it may also be the only architectural model that the architect knows well enough to offer guidance on.
2. Second, it requires more thought to implement a domain-centric design. You need to know what classes belong in the domain layer, and what classes belong in the application layer rather than just throwing everything in a business logic layer.
3. Third, it has a higher initial cost to implement this architecture compared to a traditional three layer database-centric architecture. However, it typically pays itself off if the application is complex enough, and has a long enough life cycle, which is the case with most modern applications these days.

# Application Layer:

## What is a Layer?

Layers are boundaries or vertical partitions of an application designed to represent different levels of abstraction, maintain the single responsibility principle, isolate developer roles and skills, help support multiple implementations, and assist with varying rates of change. Essentially, layers are the way that we slice an application into manageable units of complexity.

## How analyse the layers of a system

Let's start with what we already know. The classic three-layer database-centric architecture. First, we have our user interface layer, which provides the user with an interface into the application. Next, we have our business layer, which contains the business logic of the application. Finally, we have our data access layer, which contains the logic to read and write to the database. This works just fine for simple CRUD applications, that is applications that perform basic Create, Read, Update, and Delete operations on data within the database; however, it doesn't work all that well with complex or rich domain models. In addition, there's a lot of ambiguity about where application level abstractions versus domain level abstractions should go. This has led to the development of a more modern four-layer domain-centric architecture. First, we have our presentation layer, which provides the user with an interface into the application. Second, we have an application layer, which embeds the use cases of the application as executable code and abstractions. Third, we have a domain layer, which contains only the domain logic of the application. Fourth, we have the infrastructure layer. Oftentimes it makes sense to divide this layer into one or more projects. For example, a common variation is to create a separate project for persistence, and a separate project for the remaining infrastructure. For example, in this diagram we have the persistence portion of the infrastructure layer, which provides the application with an interface to the database or other persistent storage. Then, we have the rest of the infrastructure layer, which provides the application with an interface to the operating system and other third party components. Finally, we have our cross-cutting concerns, which are aspects of the application that cross all the layers of the system. There are obviously a few variations on this architecture. For example, multiple user interfaces, adding a web service layer, separate projects for external dependencies, but the general structure is essentially the same.

## Application Layer:

Here we have the application layer. It implements use cases as executable code, for example, a customer searches for a product, adds it to their cart, and pays with a credit card. We structure this executable use case code as high-level representations of application logic. For example, we might have a query that searches for our product for our customer or a command that adds a product to their shopping cart. The application layer knows about the domain layer, that is, it has a dependency on the domain, but it does not know about the presentation, persistence or infrastructure layers. That is, there are no dependencies on the outer layers of the application. The application layer, however, does contain interfaces for its dependencies that these respective outer layers then implement. Then we use an IoC framework, that is an Inversion of Control framework, and dependency injection to wire up all the interfaces and their implementations at run time.

## Layer Dependencies:

In addition to the dependency arrows in our diagram in orange I've also added additional arrows in blue to indicate the flow of control through the application. We can follow the flow of control through the application from the users at the top of the diagram down to the database and operating system at the bottom of the diagram. In addition, we can follow the dependency arrows, which flow both upwards and downwards towards the domain. Now some of you may be wondering if I've accidentally drawn two of these orange dependency arrows in this diagram upside down, specifically, the arrow between the persistence project and the application project, and the arrow between the infrastructure project and the application project. Now this is a completely reasonable assumption because it's probably quite different than what you've seen with the classic three layer architecture. However, these inverted dependencies are in fact correct. When building this more modern architecture we utilize the dependency inversion principle. It states that abstraction should not depend on details, rather, details should depend on abstractions. So in the persistence and infrastructure layers we implement the inversion of control patter. That is, our dependencies oppose the flow of control in our application. This provides several benefits, such as providing independent deployability. That is, we can replace an implementation in production without affecting the abstraction that it depends upon. It also makes architecture more flexible and maintainable as well. For example, we can swap out our persistence medium and infrastructure dependencies without having negative side effects ripple throughout both the application and domain layers. This is highly useful for agile applications where we often defer implementation decisions as late as possible when we have a much better understanding of the specific needs of our application and its implementations. This is a strategy referred to in Agile software development as the last responsible moment, an idea which we'll talk more about in the last module of this course. Please also note that sometimes we need to add an additional dependency from the persistence project directly to the domain project when using an Object Relational Mapper, that is an ORM. This is the dashed orange arrow in the diagram on the right. This is necessary for the ORM to map domain entities to tables in the database since the persistence layer needs to know about the entities contained in the domain layer. Using an ORM is optional when creating clean architecture, but it can save a tremendous amount of development, time, and effort if used correctly. Here's a quick visual example to show how all of these classes and interfaces are all wired together in our demo application. We have our presentation project, application project, domain project, persistence project, infrastructure project, and our cross-cutting concerns project. In the presentation project we have a SalesController that has a dependency on the ICreateSalesCommand interface in the application project. The CreateSaleCommand class in the application project implements this interface. This class contains the high level application logic that fulfills the use case for creating a new sale. The class has a dependency on the IDatabaseService interface and the IInventoryService interface, both of which are contained in the application project as well. The DatabaseService class in the persistence project implements the IDatabaseService interface, and the InventoryService class in the infrastructure project implements the IInventoryService interface. As we can see, all of the dependencies point towards the application, and thus transitively towards the domain. All of the details, that is implementations, depend upon abstractions, which are interfaces, and we utilize in version of control for both the persistence projects dependency on the on the application project, and the infrastructure project's dependency on the application project as well. Our cross-cutting concerns are a bit different, as there are typically multiple projects that all have dependencies upon them. For our cross-cutting concerns we store both the interfaces and the implementations in the cross-cutting concerns project. For example, the IDateService interface and the DateService class, which implements this interface, are both contained in the cross-cutting concerns project. This is because multiple projects need to reference the IDateService interface, so it must be contained in the cross-cutting concerns project referenced by all of the other projects. So why would we want to implement an application layer in our architecture? First, with an application layer we're placing focus on the use cases of the system, which, as we've stated before, are essential to the primary inhabitants of the architecture, that is the users. This provides us with the same benefits that we discussed in the first module of this course. Second, we embed our use cases as high level, executable code, which then delegate low level steps to other classes. This makes it very easy to understand the code, the use cases intention, and to reason about the software as a whole. This is highly beneficial for developers creating the application, and the developers maintaining the application. Third, it follows the Dependency Inversion Principle, which, as we explained earlier, makes our code more flexible and maintainable. In addition, it allows us to defer implementation decisions until later in the project, and thus evolve the architecture over time. There are also several reasons why we might not want to implement an application layer. First, the primary reason is that there's an additional cost to creating and maintaining this layer. Layers in software architecture are expensive to create and maintain, so we generally want to keep the number of layers in our system as small as possible. Second, we need to spend extra time thinking about what belongs in the application layer, versus what belongs in the domain layer, rather than just throwing it all in a business logic layer. Third, the inversion of control between the application layer and the infrastructure layer is often counterintuitive for novice developers. However, after you've wrapped your brain around it, it becomes quite a bit easier to reason about. So now let's take a look at how the application layer is implemented in our clean architecture demo.

# Commands and Queries:

Back in 1988 Bertrand Meyer taught us that there were two kinds of methods in object oriented software. First, we have a command. A command does something, which means that it should modify the state of the system, but it should not return a value. Next, we have queries. A query answers a question, which means that it should not modify the state of the system, and it should return a value. Bertrand taught us that we should attempt to maintain command query separation where possible. There are many reasons why this is a good idea. For example, to avoid nasty side effects that hide in methods that violate this principle.

## Command-Queries separation exception:

As Martin Fowler points out, this is not always possible. For example, if you have a stack, and you want to pop the first item on the stack, the pop method removes the top item from the stack, which is a command, and returns that top item, which is a query. In addition, if you want to create a new database record you create the database record, which is a command, but you might also need to return the newly created ID, which is a query, so there are clearly exceptions to this rule, but in general we should strive to maintain command query separation where possible.

## Command Query Responsability Separation (CQRS):

Command Query Responsibility Separation architectures or CQRS architectures expand this concept of command query separation to the architectural level. In general, we're dividing the architecture into a command stack, and a query stack, starting at the application layer. This is done for various reasons. The primary reason is that queries should be optimized for reading data, whereas commands should be optimized for writing data. Commands execute behaviors in the domain model, mutate state, raise events, and write to the database. Queries use whatever means is most suitable to retrieve data from the database, project it into a format for presentation, and display it to the user. This change increases both the performance of the commands and queries, but equally important, it increases the clarity of the respective code. CQRS is domain-centric architecture done in a smart way. It knows when to talk to the domain via commands, and when to talk directly to the database via queries. There are three main types of CQRS, which we'll take a look at next.

### Single-database CQRS

The first type of CQRS, we'll call it single-database CQRS for lack of an existing standardized name. This type of CQRS has a single database that is either a third normal form relational database or some type of NoSQL database. Commands execute behavior in the domain, which modify a state, which is then saved to the database through the persistence layer, which is often an ORM, that is an Object Relational Mapper like in Hibernate or Entity Framework. Queries are executed directly against the database using a thin data access layer, which his either an ORM using projections, LINQ to SQL, SQL scripts or stored procedures. This single database CQRS is the simplest of the three types of CQRS architectures.

### Two-database CQRS

The second type of CQRS architecture we'll call Two-database CQRS. This type of CQRS has both a read database and a write database. The command stack has its own database optimized for write operations. For example, a third normal form relational database or a NoSQL database. The query stack, however, uses a database optimized for read operations. For example, a first Normal Form relational database or some other denormalized read optimize data store. The modifications to the right database are pushed into the read database either as a single coordinated transaction across both databases or using an eventual consistency pattern. That is, the two databases may be out of sync temporarily, but will always eventually become in sync, typically on the order of milliseconds. This type of CQRS is more complex than the first, but can afford orders of magnitude improvements in performance on the read side of the system. This makes quite a bit of sense because we generally spend orders of magnitude more time reading from a database than we do writing to it.

### Event sourcing

The third type of CQRS system is typically referred to as event sourcing. The main difference here is that we do not store the current state of our entities in a normalized data store. Instead, we store just the state modifications to the entities over time, represented as events that have occurred to the entities. We store this historical record of all events in a persistence medium called an event store. When we need to use an entity in its current state we replay the events that have occurred to that entity, and we end up with the current state of the entity. Then, once we've reconstructed the current state of the entity we execute our domain logic, and modify the state of the entities accordingly. This new event will then be stored in our event store, so that it can be replayed as needed. Finally, we push the current state of our entity out to the read database, so our read queries will still be extremely fast. This is the most complex of the three types of CQRS, but it has some very powerful benefits in exchange for the additional costs. First, since the current state of each entity can only be derived by replaying the sequence of events that have occurred to that entity, the event store acts as a complete and guaranteed to be true audit trail for the entire system. This is highly valuable in heavily regulated industries where this type of auditability is necessary. Second, we can reconstruct the state of an entire entity at any point in time. This is useful for determining what the state of an entity was at any previous point in time in the system, and this is also very useful for debugging. Third, we can replay events to observe what happened in the system. This is very useful for diagnostics and debugging. In addition, this is also very useful for load testing and regression testing in a test environment using existing production events that have occurred in the system. Fourth, we can project the current state of our entities into more than one type of read optimized datastore. For example, we can simultaneously populate and then query fast text indexing services like Lucene, graph databases, OLAP cubes, In-memory databases, and more. This means that each query can request data from the datastore optimized for querying and presenting the corresponding data. Finally, we can rebuild our production database just by playing the events. All we need to do to get the current state of our system is replay all of the events that have occurred in the past, and we end up with the current state of the system. These are some very powerful features if your architecture needs them; however, it can also be a significant additional expense if you don't actually need any of these features. In addition, despite the fact that most people new to event sourcing assume that the right side of the system will be slow, in reality these systems are actually much faster than you would imagine.
There are also several types of optimizations that can be added, like generating periodic snapshots, if the performance of the right side of the system becomes an issue.

## why would we want to use a CQRS architecture?

First, if you're implementing domain-centric design, implementing CQRS is more efficient from a coding perspective. Commands are coded to use the rich domain models to modify state, and queries are coded directly against the database to read data. Second, by using CQRS we're optimizing the performance of both the query side and the command side for their respective purposes. Depending upon which type of CQRS we implement, this can mean orders of magnitude improvements and performance. Third, by using event sourcing we gain all of the benefits we discussed previously, and a few more that were not discussed. As systems become more complex or require high degrees of auditability these features can become highly valuable to both the business and to the developers. So why would we not want to use CQRS? First, there's an intentional inconsistency in the design of the command stack versus the query stack. Inconsistency in general makes software more complex and difficult to reason about; however, we gain consistency within each stack as a tradeoff. Second, with two-database CQRS, having both a read and write database is more complex, and potentially introduces an eventual consistency model to your databases. Third, event sourcing entails higher cost to create and maintain the event sourcing features. If you'll not be deriving sufficient business value from these additional features event sourcing might not pay for itself in the long run. So now let's take a look at the query side of our demo application.

# Functional Organization:

"The architecture should scream the intent of the system" - Uncle Bob.

The screaming architecture practice is based on the idea that your software's architecture should scream the intent of the system, hence the name screaming architecture. We do this by organizing our architecture around the use cases of the system. Use cases are representations of a user's interaction with the system. For example, interactions like getting a list of all customers, purchasing a product or paying a vendor. The screaming architecture practice is best explained using a metaphor about the architecture of buildings. Let's take a look at this blueprint. We have some bedrooms, a dining room, a living room, kitchen, and a bathroom. It's pretty easy for us to determine the intent of this architecture by quickly scanning across the blueprint. This is clearly the blueprint for a residential building of some kind, and the intent of this architecture is to facilitate a residential living environment. The rooms of this building embody the use cases of the building. We sleep in a bedroom, we cook in a kitchen, we eat in a dining room, and so on. Simply by looking at the rooms contained in an architectural blueprint, which represent the use cases of the building, we can quickly determine the function and intent of the architecture of the building. Now rather than looking at the blueprints for a building let's take a look at the bill of materials for a building instead. We can see that we have some appliances, cabinets, doors, fixtures, and more, but it's very difficult to determine the intent of this architecture. By looking at a list of components used to create the building, rather than the rooms that support the building's use cases, it's much more difficult to determine the function or intent of the architecture of a building. The relationship between the organization of software architecture and the ease of discovering the intent of the architecture is governed by similar principles. We can organize our application's folder structure and namespaces according to the components that are used to build the software, components like models, views, and controllers or we can organize our folder structure and namespaces according to the use cases of the system, concepts that pertain to user's interactions with objects in the system like customers, products, and vendors. For example, let's take a look at two representations of the same software architecture. On the left we have the typical MVC folder structure. Things we all recognize as MVC components, like models, views, and controllers. On the right, however, we have the same web application organized by its high-level use cases like customers, products, and vendors. It's very difficult to determine the intent of the software on the left, but it's much easier to determine the intent of the software on the right. This might sound like an arbitrary decision, whether to organize first by components or by use cases, but there are some definite pros and cons to both of these equally reasonable ways to organize our software. The primary advantage is the functional cohesion; that is, organizing by use cases is generally more efficient than categorical cohesion; that is, organizing by component types because it better models the way we maintain, navigate, and reason about software. Essentially, categorical cohesion is like storing your silverware forks next to your pitch forks and tuning fork just because they're all three forks of some kind, whereas functional cohesion is like storing your forks next to your knives and spoons because we use all three utensils when we're eating. In fact, we can extend the idea of using blueprints for our building architecture metaphor to both visualize and conceptualize our application architecture. Here we have a tree map of the application layer of our demo application. A tree map is a data visualization that allows us to visualize hierarchical data, like the hierarchical relationship between the classes, folders, and namespaces in our demo application. The main rectangle on the screen represents the application project in our overall solution. Each of the large, colored rectangles represents the first folder level within the application project; that is, the aggregate root entity folders. The smaller rectangles represent the individual classes and interfaces contained in those aggregate root folders. The size of each rectangle corresponds to the size of the respective class or interface file that it represents. The colors correspond to each aggregate root entity folder that the classes and interfaces are contained within. As we can see by looking at the blueprint of our code, it is very easy to determine what the function and intent of this application is. In addition, it's easy to see that the items that are used together are grouped together via functional cohesion. This is a principle referred to as special locality; that is, it is often more efficient to keep items that are used together near one another in physical space. In terms of our software project, this means keeping items that are used together near one another in the folder structure of our file system. In fact, we can extend this metaphor one step further and imagine that our code is a multi-story building with each floor representing a layer of the architecture. On the ground floor we have our infrastructure layer, which contains both persistence and the remaining infrastructure classes. On the second floor we have our domain layer, which contains our domain entities. On the third floor we have our application layer, which contains our use cases, and on the top floor we have our presentation layer, which contains our user interface. When we conceptualize and visualize our software this way it becomes pretty apparent to us what the organization of a clean architecture looks like, versus what a messy architecture looks like. Simply by imagining having to live and work inside of this hypothetical building made of code we can get an intuition for whether the organization of our architecture is inhabitable or not.

## Why would we want to use functional organization.

So why would we want to use functional organization in our architecture? First, when we organize by function we utilize the principle of spatial locality; that is, items that are used together live together, just like our forks, knives, and spoons. Second, it's much easier to find things and navigate the folder structure. If we want to work with the employee objects, like the employee models, views, and controllers we just navigate to the employees folder in the presentation layer, and they're all contained in that folder. Third, it helps us to avoid vendor and framework lock in because we're not forced into the folder structure that the vendor insists that we use to implement their framework.

## Why would we not want to use functional organization.

So why would we not want to use functional organization in our architecture? First, we lose the ability to use the default conventions of our frameworks, so we typically have to tell the framework where things are located. Second, we often lose the automatic scaffolding features of our frameworks because they are typically designed to create automatically generated template files by categorical cohesion, since it's easier for the frameworks. Third, categorical cohesion is easier during the creation process; that is, figuring out where something should go when you created it, however, it makes things more difficult over the rest of the life of the project, so you'll typically end up paying more over the life of the project, especially if the project is complex and has long lifecycle. So now let's see how we've used functional organization to organize the classes, folders, and namespaces of our demo application.

# Microservices:

## Context

First, let's start with what we already know, components. Components are how we would typically subdivide the layers of our architecture once it has grown beyond a manageable size. We typically implement components as separate projects within our overall solution, then when we build these projects we create output files like DLLs and C#, assemblies in. NET, jar files in Java, and gem files in Ruby. This allows us to work on the components individually, integrate them as necessary, and deploy them independently. Our users interact with the application typically through a composite user interface, which calls the appropriate component stack, and presents the user interface as a unified view of the system. In addition, all of the data for each component stack are typically stored in a single database. Let's assume, hypothetically, that we're building an application to perform sales and support tasks. Our problem domain contains the following nouns. On the sales side we have a sales opportunity, a contact for the sales opportunity, a sales person, a product to be sold, and a sales territory. On the support side of the problem domain we have a support ticket, a customer, a support person, a product being supported, and a resolution to the support ticket. In reality, there would likely be several other nouns in both columns, but we'll just keep this list short to make this example more manageable.
In the old days we would have modeled this just like we'd been traditionally taught. We would have created a single unified domain model by merging the nouns that pointed to the same types of physical objects and mental concepts as a single entity. For example, a product is a product whether it's on the sales side or the support side, so we would model that as a single product entity. A contact becomes a customer on the support side, so we would have modeled that as a single customer entity with a bunch of nullable fields for when it was being used as a contact. A sales person and a support person are both employees, so we'd represent them as a single employee entity, and this strategy seems reasonable enough; however, as we try to model larger and larger domains it becomes progressively harder to create a single unified domain model. The problem is that models are only applicable within a specific context. When we try to force a model to work within multiple contexts things just don't feel right. We have extra unused properties, methods, and validation rules that are applicable in one context, but not in other contexts. In addition, we have variations on terminology. For example, do we have a sales person, a support person or is this just an employee?

## Bounded Context

A bounded context is the recognition of a specific contextual scope within which a specific model is valid. We constrain our domain model by each bounded context, and subdivide our models appropriately. Then we communicate state transitions of our cross boundary models from one domain to the other. We do this through clearly defined interfaces using either coordinated transactions or an eventual consistency model. This leads us to microservices.

## What does a microservice architecture?

Microservice architectures subdivide monolithic applications; that is, the divide a single, large application into smaller subsystems. These microservices communicate with one another using clearly defined interfaces, typically over lightweight web protocols, for example, JSON over HTTP via rest APIs. Microservices can also subdivide larger teams into smaller development teams; that is, one team for each microservice or set of microservices. These services are also very independent of one another. Each one can have its own persistence medium, programming language, architecture, and operating system. In addition, you can independently deploy each microservice and independently scale them as needed, which is very beneficial for cloud scale applications. I should also note that microservices are similar in concept to service oriented architectures; however, they don't explicitly prescribe the use of an Enterprise service bus, along with several other key differences. Two very frequent questions that often come up when discussing microservices are how big should each microservice be, and where should I draw the boundaries of each microservice? This is where bounded contexts come in. Microservices have a natural alignment to the boundaries of bounded contexts. Ideally, in most cases we want each domain, each microservice, each database, and each corresponding development team to line up. Doing so provides several benefits. For example, this maximizes the cohesion of domain entities in each bounded context and microservice, and it minimizes the coupling relative to any other way we could have partitioned the system. In addition, this allows each team to focus on only a single domain of knowledge. They don't need to know about the intimate details of any other team's domain, database or microservices. They just need to know how to communicate with the well-defined interfaces of those other microservices. This also means that each domain and microservice has a consistent data model; that is, all entities contained within a bounded context are consistent with one another, even if they are temporarily inconsistent with other microservices using an eventual consistency model. In addition, within each microservice each team can use whatever technologies, patterns, practices or principles work best to solve the specific business problems of their domain according to the specific constraints and business objectives of their respective microservices. This could entail using different architectures, persistence mediums, programming languages or even operating systems for each individual microservice based on the specific needs of that domain, and the knowledge and skills of each development team. This is highly valuable for agile software development teams using agile practices to develop large business applications. However, I should note that there's still a lot of debate in the industry as to how small microservices should be. Some experts advocate that they should be smaller than I'm suggesting, while others think that they should be larger. In addition, there's debate over where to draw the boundaries of each microservice. For example, a slightly different approach for organizing microservices is to have a single microservice for each aggregate root within each bounded context, but then have all of those microservices talk to a single database for their respective bounded context. However, information is still exchanged between bounded context via each bounded context set of microservices.

## Pros of microservices

There are definite pros and cons to both of these equally reasonable ways to organize your microservices; however, it's up to each of you to decide what makes the most sense for your project and architect your microservices accordingly. If we think back to our metaphor in module one of bad architecture looking and feeling like spaghetti and good architecture looking and feeling like lasagna, then microservices should look and feel like ravioli to us. Each raviolo, which is the singular form of the word ravioli, is a self-contained package of Italian food. They maintain high cohesion and low coupling, they each hide their internal contents from one another, and you can swap one raviolo with another without disrupting the internal contents of the other ravioli. There are clearly advantages to eating pasta packaged this way, relative to just globbing one giant mess of meat, sauce, and dough on someone's plate. Similarly, there are clear advantages to using microservices over monoliths in certain contexts, subject to various constraints and business objectives as well.

# So why would we want to use microservices?

1. the cost curve of microservices is flatter than the cost curve of monolithic applications as a function of the size of the system being built. That is, the cost to build microservices is initially higher than monoliths for small systems, but the cost of building microservices grows much more slowly as the system gets bigger, so for projects with large domains and sufficiently long life cycles, using microservices in this context can, in theory, reduce the overall cost of the project.
2. subdividing a monolithic application into microservices based on bounded contexts creates systems with high cohesion and low coupling. This isn't just in terms of the cohesion and coupling in our code base, but also high cohesion and low coupling for our development teams, domain knowledge, database models, and more.
3. microservices offer independence. Essentially, you can use whatever technologies, design patterns, and development practices are most appropriate for each specific microservice and corresponding team.

## So why would we not want to use microservices?

1. microservices have a higher up front cost than a single domain. It's not until later in a large software project that the cost curves intersect and microservices eventually become less expensive than monoliths. If we have a small domain, a small total team size or a short project life cycle, then creating a set of microservices could likely be more expensive than creating a single application with a single domain.
2. there's a phenomena observed in the software industry referred to as Conway's' Law. To paraphrase, it states that organizations which design systems are constrained to produce systems that mirror the communication structures of their organizations. So organizations that are top-down, command and control bureaucracies, like traditional waterfall project teams, are more likely to produce top-down command and control monolithic applications; whereas, organizations that are bottom-up, self-organizing adhocracies like agile project teams, are more likely to produce bottom-up, peer-to-peer microservice architectures. If your organization and team are not set up in a manner that can communicate and coordinate effectively in the way that microservices entail, your organizational structure will likely experience significant resistance to creating microservice architectures.
3. there are additional costs in building distributed systems. You spend extra time and effort when building distributed systems dealing with network latency, fault tolerance, load balancing, and more. This is why most experts strongly advise first starting with a single application, and only break the application into microservices when the pain of dealing with a monolithic application becomes greater than the additional cost and complexity of an equivalent set of microservices. For me, this point in time almost always occurs when you try overlapping multiple bounded contexts into a single monolithic application. So now let's take a look at how our demo application communicates with a microservice.

# Testable Architectures:

## the current state of testing in the software industry.

despite great advances in both testing practices and technology to enable software testing, many software developers still do very little automated testing of their code, and when they do it's relatively ineffective or highly inefficient. There are numerous reasons given for why developers don't create high quality automated tests for their code. These reasons include not having enough time to create tests, it's not my job to test software, testing is for testers not developers, and it's too hard to create good tests because our architecture makes testing difficult. While there are compelling counter arguments to each of these claim, and more, we're going to be focusing on the last of these three arguments during this course. We're going to show how clean architecture actually makes testing easier not harder. In addition, how test driven development drives the design of a clean architecture. These two forces work side by side to create a feedback loop that feeds into one another.

## Test-Driven Development

Test-Driven Development is a software practice where we create a failing test first before we write any production code, and use this test to drive the design of the architecture. We refer to this three-step process of Test-Driven Development as red, green, and refactor. First, we start by creating a failing test for the simplest piece of functionality we need to create. This is the red step of the TDD process. Next, we implement just enough production code to get that failing test to pass. This is the green step of the process. Then, we refactor our existing code; that is, we improve both the test and production code to keep the quality high. This is the refactor step in the process. We repeat this cycle for each piece of functionality in order of increasing complexity, in each method and class until the entire feature is complete. By using the Test-Driven Development practice we are creating a comprehensive suite of tests that covers all code passive importance in our application. In addition, by using TDD the design of each of these classes and methods is being driven by the testing process. This means that all classes and methods will be easily testable by default, since the tests are driving the design. In addition, this coincidentally makes our classes and methods more maintainable because of an interesting parallel in the physics of cohesion and coupling with both testability and maintainability. Essentially, by creating testable code we are coincidentally creating more maintainable code. More importantly, this comprehensive suite of tests eliminates fear, fear that making changes to our code will cause regression errors in our software. If we can eliminate the fear of change in our architecture we are more likely to embrace change and keep our architecture clean. While I'm personally a very strong advocate of Test-Driven Development, and would really like to cover it here extensively, unfortunately we'll have to defer a more in-depth discussion of TDD to another course. What's important to note for this course is that Test-Driven Development is a key component to creating a clean and testable architecture. There are a variety of types of tests that exist in the world of software testing. Some tests are based on what they are testing. For example, unit tests, integration tests, component tests, service tests, and coded UI tests. Some are based on why they are being tested. For example, functional tests, acceptance tests, smoke tests, and exploratory testing. Still, others are based on how they are being tested. For example, automated tests, semi-automated tests, and manual tests. While we don't have time to cover all the various types of tests that can be applied to our software architecture, we will, however, cover a subset of the most common of these types of tests, and how our clean architecture makes them easier rather than harder to create and maintain.

## Types of tests

In the book, Succeeding With Agile, Mike Cohn describes a concept he referred to as the test automation pyramid. The test automation pyramid. The test automation pyramid identifies four types of tests.

- First, we have unit tests, which are automated tests that verify the functionality of an individual unit of code in isolation.
- Second, we have service tests, which are automated tests that verify the functionality of a set of classes and methods that provide a service to the users.
- Third, we have coded UI tests, which are automated tests that verify the functionality of the full application from the user interface down to the database.
- Finally, we have manual tests, which are tests performed by a human that verify the functionality of the full application as well.

The test automation pyramid captures the essence that each type of test becomes more costly the further up the pyramid we go. As a result, we want to have a large number of low cost tests and a small number of high cost tests. For example, unit tests are relatively quick and easy to create, run extremely fast, rarely fail due to false positives, and are relatively inexpensive to maintain, so they are much less costly than the other types of tests. However, coded UI tests are much more difficult and time consuming to create. They also run much slower, they're more brittle, relatively unreliable, and relatively more difficult to maintain. So they are relatively more costly than other types of tests lower on the pyramid. Given this, we want to maximize the return on investment from our testing efforts by creating the right balance of each type of test relative to the cost of the test, and the benefit that the test will provide, so we should anticipate creating lots of small, low cost unit tests, some medium cost service tests, a few high cost coded UI tests, and very few repetitive manual tests. Doing so, in theory, gives us the most bang for the buck for our testing efforts.

## Acceptance Tests.

A final type of test that I'd like to discuss is a type of test called an acceptance test. Acceptance tests verify the functionality of the application like the other tests do; however, the main difference is that they are typically written in the language of the business, and used as the criteria by the product owners and product stakeholders to determine when a certain feature is considered complete and functioning as expected. These tests are often done either as manual tests or as coded UI tests. However, using manual tests and coded UI tests for acceptance testing is problematic for the various reasons we discussed earlier, so one place where clean architecture really helps us make testing easier, and get the most value out of our acceptance testing, is by allowing us to replace high cost manual encoded UI acceptance tests with automated service level acceptance tests. This is possible with clean architecture because all of our application and domain logic can be tested in isolation. To do so, first, we eliminate the user interface from our acceptance tests. We do this by having our acceptance test work directly with the commands and queries in our application layer. Next, we eliminate the database from our acceptance tests as well. We do this by mocking out the real database with a fake in-memory database instead. Then, we mock out the operating system and any external dependencies in the infrastructure layer. We do this by replacing the dependencies with fake test doubles called mocks that act as surrogates for the real dependencies. We also do the same for our cross-cutting concerns. Acceptance criteria should be written in the language of the business to describe business functionality that the application needs to provide. They should focus on what is essential to the business, and not implementation details. Thus, implementation details like presentation, databases, third party dependencies, etc., should all be irrelevant to these acceptance criteria. In addition, because we've kept our presentation, persistence, and infrastructure projects thin; that is, they contain minimal logic, and no application or domain logic, eliminating them from our acceptance tests should pose little risk from the standpoint of verifying business functionality; however, there can be exceptions, for example, verifying non-functional requirements, security requirements or auditing requirements, but we can handle verifying these types of non-functional requirements in other ways. With this style of acceptance testing our acceptance tests are focused on what is essential in our application; that is, the business domain, and the use cases in the application layer, not the implementation details like databases, operating systems, third party dependencies or cross-cutting concerns. Doing this allows us to minimize the number of coded UI tests in our application, so rather than using coded UI tests for acceptance testing, we can instead reserve them for what are called smoke tests. Smoke tests are a very small number of simple, full-system tests that just verify that the application actually runs, and nothing more when all the pieces are assembled at runtime. This reduces the cost of creating and maintaining a large number of these costly and complex coded UI acceptance tests, and reduces the likelihood of false positives, which degrades our confidence in our comprehensive suite of tests. In addition, this also allows us to minimize the number of manual acceptance tests as well. Thus, we reduce our manual testing costs, and free up our testers to do much more valuable and rewarding work, like exploratory testing, and ensuring a high quality user experience for our end users.

## So why would we want to use a testable architecture?

- First, by applying these testable architecture practices we make our code easier to test, and if our code is easy to test we'll be more likely to create and maintain these tests. We want our architecture to encourage developers to write tests and to practice Test-Driven Development.
- Second, creating testable architecture improves the design of our architecture. This is because the physics of cohesion and coupling of testable code parallels that of maintainable code. So, by virtue of adopting Test-Driven Development, and creating a testable architecture we're actually creating a more maintainable architecture as a result.
- Third, by creating a comprehensive suite of tests for our architecture we eliminate fear. By eliminating the fear that changes to our code will break the code we're much more likely to embrace change, continuously refactor our code to improve it, and keep our architecture clean. So why would we not want to create testable architecture? Honestly, there are very few reasons I can think of to not want to create testable architecture.

## why would we not want to use a testable architecture.

In fact, testability is probably one of the biggest selling points, in my opinion, for adopting clean architecture practices. However, for the sake of counter-argument, I'll do my best to articulate a few cases where you might not want to create a testable architecture.

- First, there's an initial higher upfront cost to using Test-Driven Development and creating testable architecture, so for a very small project or for disposable software projects, for example, a simple throw away console application with clear and stable requirements, it might be more cost effective to just build the tool and test its narrow functionality by hand.
- Second, Test-Driven Development requires practice and discipline. It's not something you just learn in a day, and it takes quite a bit of practice to become highly proficient with TDD. In addition, it requires a lot of discipline to keep yourself from falling back into your old coding habits.
- Third, testable architecture practices usually require buy-in from the whole team. If the whole team isn't creating testable code, running all of the tests, and maintaining these tests, then the test and the testable code will eventually decay and become obsolete, so while I personally think it's very unfortunate, some teams simply just are not interested in creating automated tests or writing testable code.

# Evolving the architecture.

What we've seen so far in this course isn't the end goal of this set of architectural patterns, practices, and principles; rather, it's just the beginning. What I've attempted to define is a starting point for building modern applications that will benefit from this set of practices. These are typically applications built using Agile software development practices in an environment with a high degree of risk due to uncertainty and changing requirements caused by changing technologies, changing markets, and changing user preferences. This means that the architecture needs to evolve to minimize this risk due to uncertainty, and to meet these changing requirements. By placing focus on the key abstractions of the domain and application logic at the center of the architecture, and deferring user interface, persistence, third party dependencies, and cross-cutting concerns to implementation details clean architecture allows the application to more easily evolve over time. When we're making these implementation decisions we want to defer these decisions until the moment known as the last responsible moment.

## Last responsible moment

This term was coined by Mary and Tom Poppendieck in the book, Lean Software Development: An Agile Toolkit. The last responsible moment is a strategy for avoiding making premature decisions by deferring important and difficult to reverse decisions until a point in time where the cost of not making the decision becomes greater than the cost of making the decision. By delaying these decisions until the last responsible moment we increase the likelihood that we are making well informed decisions because we now have more information. Making implementation decisions too early can be a huge risk on some projects; however, waiting until after the last responsible moment, as the name implies, creates potential risks and technical debt as well. Evolutionary architecture practices are about creating architecture that allows us to more easily defer these decisions until the moment where we've minimized the risk due to making the decision too early, but not waited too long, and accumulated technical debt. For example, by focusing on the domain and application logic, and solving the key problems that our users need to solve, we can validate whether or not our software will actually provide real business value to our users before we invest heavily in the implementation details. In the meantime, we can just use the simplest solution that could possibly work for each of the implementation details, just enough to get us by while we validate the primary value proposition of the application; that is, whether or not our software will provide real business value to our users and our business. In addition, technology will likely change over the life of the project. For example, during the course of a project a new type of database technology may eventually be released, that makes more sense than using the database technology that we'd previously decided upon. Clean architecture makes it much easier to defer implementation decisions or replace existing implementations than an architecture built around a specific implementation. Markets may also change over the life of a project as well. For example, a simple client server application designed to support a few hundred users might balloon in demand to millions of users and need cloud scale, CQRS, and microservices to support the new workload. With clean architecture we build a solid domain and application core first, and then scale that core as large necessary, if and when the need should arise. User preferences may also change over the life of a project as well. For example, we might discover later on in the project that users now want a mobile user interface rather than the desktop user interface that they had previously desired. With this style of architecture we can easily swap out presentation technologies or support multiple user interfaces built upon the key abstractions in the domain and application layer. We want our application architecture to be flexible by default to help protect us against the unpredictability of the future. These architectural patterns, practices, and principles that we've just learned give us this kind of flexibility and adaptability.

## So why would we want to create an architecture that evolves over time.

- First, evolutionary architectures embrace uncertainty. If you're building an application in an environment with a high degree of uncertainty it's better to have an architecture that can evolve as you learn more over time than to try to predict the future and build a rigid architecture based on those, likely to be wrong, predictions.
- Second, evolutionary architectures embrace change. It's inevitable that on most software projects the requirements will change over the life of the project. Having a flexible architecture allows the architecture to adapt to these changes.
- Finally, evolutionary architectures help us to reduce certain types of risk. If uncertainty or changing requirements are the biggest risk to your project, which has been the case with most projects I've worked on over the years, then optimizing your architecture for adaptability helps you to reduce this risk.

## So why would we not want to build an evolutionary architecture?

- First, if you have a project with very clear requirements, and you've already validated each of those requirements, then there would likely not be much uncertainty, and thus, minimal need to change the application as we learn more.
- Second, if you have staple requirements; that is, requirements that will not change over the life of the project, then the value of an adaptable architecture would likely not be justified by this cost. However, I feel compelled to point out that I have yet to see a project of any significant complexity in over 17 years of professional experience that has met both of these first two criteria. This may be the case with very small, simple, and disposable applications, but it's unlikely to be the case with large and complex applications with a long lifecycle. Finally, it's important to be aware that there are obviously limitations to the flexibility of clean architecture. It can't work miracles, but it's still significantly more adaptable and maintainable than any of the more traditional styles of architecture I've worked with over the years.
